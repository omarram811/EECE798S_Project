[
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C1+-+Introduction+to+GenAI.pdf",
    "question": "What is the main difference between discriminative and generative AI?",
    "gold_answer": "Discriminative AI predicts labels or class boundaries, while generative AI models the data distribution to generate new content.",
    "answer": "Discriminative AI focuses on predicting labels from inputs—learning decision boundaries between classes. Generative AI models the data distribution itself to create new content, producing novel examples like text, images, or music.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C1+-+Introduction+to+GenAI.pdf",
        "C1+-+Introduction+to+GenAI.pdf",
        "C1+-+Introduction+to+GenAI.pdf",
        "C1+-+Introduction+to+GenAI.pdf",
        "C1+-+Introduction+to+GenAI.pdf"
      ],
      "top1_doc": "C1+-+Introduction+to+GenAI.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 2
        },
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 1
        },
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 4
        },
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 0
        },
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 8
        }
      ]
    },
    "metrics": {
      "f1": 0.5531914893617021,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.9231194853782654,
      "latency_seconds": 6.883841900000334,
      "input_tokens": 1446,
      "output_tokens": 242,
      "total_tokens": 1688,
      "input_cost": 0.0018075,
      "output_cost": 0.00242,
      "total_cost": 0.0042275
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C1+-+Introduction+to+GenAI.pdf",
    "question": "What was the technological shift that enabled modern AI agents?",
    "gold_answer": "The shift was the emergence of powerful generative models—especially transformers and LLMs—that provided agents with strong reasoning and language abilities.",
    "answer": "The shift was the rise and scaling of generative AI—especially large language models based on the Transformer/attention architecture—which gave agents strong language understanding, reasoning, and multi-step planning. In short, modern LLMs unlocked true agentic behavior.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C1+-+Introduction+to+GenAI.pdf",
        "C1+-+Introduction+to+GenAI.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C3+-+Introduction+to+Agents.pdf"
      ],
      "top1_doc": "C1+-+Introduction+to+GenAI.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 1
        },
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 0
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 0
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 1
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 2
        }
      ]
    },
    "metrics": {
      "f1": 0.40816326530612246,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.8529139757156372,
      "latency_seconds": 7.730361100000664,
      "input_tokens": 1657,
      "output_tokens": 443,
      "total_tokens": 2100,
      "input_cost": 0.00207125,
      "output_cost": 0.00443,
      "total_cost": 0.00650125
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C1+-+Introduction+to+GenAI.pdf",
    "question": "Name two real-world applications of generative AI discussed in the document.",
    "gold_answer": "Image generation and visual question answering.",
    "answer": "- Agentforce by Salesforce: an AI platform for creating and managing autonomous agents that provide 24/7, proactive support to employees and customers.\n- Cursor’s Agent mode: an autonomous agent that explores codebases, reads documentation, and runs terminal commands to complete tasks.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C1+-+Introduction+to+GenAI.pdf",
        "C1+-+Introduction+to+GenAI.pdf",
        "C1+-+Introduction+to+GenAI.pdf",
        "C1+-+Introduction+to+GenAI.pdf",
        "C1+-+Introduction+to+GenAI.pdf"
      ],
      "top1_doc": "C1+-+Introduction+to+GenAI.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 4
        },
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 3
        },
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 2
        },
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 1
        },
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 0
        }
      ]
    },
    "metrics": {
      "f1": 0.05263157894736842,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.28734996914863586,
      "latency_seconds": 9.442254999999932,
      "input_tokens": 1503,
      "output_tokens": 447,
      "total_tokens": 1950,
      "input_cost": 0.00187875,
      "output_cost": 0.00447,
      "total_cost": 0.00634875
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C1+-+Introduction+to+GenAI.pdf",
    "question": "What role do transformers play in modern GenAI?",
    "gold_answer": "Transformers enable models to understand contextual relationships through attention mechanisms.",
    "answer": "Transformers introduced the attention-based architecture that revolutionized natural language understanding and generation. Though not originally built for generation, they became the breakthrough backbone enabling today’s powerful generative models and modern GenAI.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C1+-+Introduction+to+GenAI.pdf",
        "C1+-+Introduction+to+GenAI.pdf",
        "C1+-+Introduction+to+GenAI.pdf",
        "C1+-+Introduction+to+GenAI.pdf",
        "C1+-+Introduction+to+GenAI.pdf"
      ],
      "top1_doc": "C1+-+Introduction+to+GenAI.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 16
        },
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 0
        },
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 15
        },
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 1
        },
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 4
        }
      ]
    },
    "metrics": {
      "f1": 0.10526315789473682,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.6974300146102905,
      "latency_seconds": 4.662353699999585,
      "input_tokens": 1528,
      "output_tokens": 241,
      "total_tokens": 1769,
      "input_cost": 0.00191,
      "output_cost": 0.00241,
      "total_cost": 0.00432
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C1+-+Introduction+to+GenAI.pdf",
    "question": "What is the definition of Generative AI according to the slides?",
    "gold_answer": "Generative AI is a field where models create new content such as text, images, audio, or video based on training data.",
    "answer": "Generative AI is a field of AI where models can create new content—such as text, images, audio, or video—based on the data they were trained on.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C1+-+Introduction+to+GenAI.pdf",
        "C1+-+Introduction+to+GenAI.pdf",
        "C1+-+Introduction+to+GenAI.pdf",
        "C1+-+Introduction+to+GenAI.pdf",
        "C1+-+Introduction+to+GenAI.pdf"
      ],
      "top1_doc": "C1+-+Introduction+to+GenAI.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 1
        },
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 3
        },
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 4
        },
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 2
        },
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 0
        }
      ]
    },
    "metrics": {
      "f1": 0.7111111111111111,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.9839206337928772,
      "latency_seconds": 3.9234278999992966,
      "input_tokens": 1502,
      "output_tokens": 172,
      "total_tokens": 1674,
      "input_cost": 0.0018775,
      "output_cost": 0.00172,
      "total_cost": 0.0035975
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C2+-+Large+Language+Models.pdf",
    "question": "What makes a language model 'large'?",
    "gold_answer": "A model is considered large when it contains millions to billions of parameters stacked across many transformer layers.",
    "answer": "A model is “large” when its architecture is scaled up—stacking many encoder/decoder blocks—so it contains millions to billions of parameters. The basic transformer is a language model, but not a large one.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C2+-+Large+Language+Models.pdf",
        "C2+-+Large+Language+Models.pdf",
        "C2+-+Large+Language+Models.pdf",
        "C2+-+Large+Language+Models.pdf",
        "C6+-+Agent+Memory.pdf"
      ],
      "top1_doc": "C2+-+Large+Language+Models.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 3
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 2
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 30
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 32
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 19
        }
      ]
    },
    "metrics": {
      "f1": 0.6222222222222222,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.8510997295379639,
      "latency_seconds": 8.085575499999322,
      "input_tokens": 1543,
      "output_tokens": 245,
      "total_tokens": 1788,
      "input_cost": 0.00192875,
      "output_cost": 0.00245,
      "total_cost": 0.00437875
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C2+-+Large+Language+Models.pdf",
    "question": "What were LLMs originally designed to do?",
    "gold_answer": "They were designed to perform text modeling tasks with high accuracy by predicting the next token.",
    "answer": "They were originally designed to model language and perform traditional text tasks with high accuracy (e.g., generation, translation, summarization, and question answering).",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C3+-+Introduction+to+Agents.pdf",
        "C2+-+Large+Language+Models.pdf",
        "C2+-+Large+Language+Models.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C2+-+Large+Language+Models.pdf"
      ],
      "top1_doc": "C3+-+Introduction+to+Agents.pdf",
      "top1_matches_gold": false,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 10
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 4
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 3
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 9
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 2
        }
      ]
    },
    "metrics": {
      "f1": 0.5405405405405405,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.7481739521026611,
      "latency_seconds": 6.309327999999368,
      "input_tokens": 1591,
      "output_tokens": 231,
      "total_tokens": 1822,
      "input_cost": 0.00198875,
      "output_cost": 0.00231,
      "total_cost": 0.0042987500000000005
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C2+-+Large+Language+Models.pdf",
    "question": "Name one emergent behavior of LLMs shown in the slides.",
    "gold_answer": "Chain-of-thought reasoning.",
    "answer": "Solving new tasks without gradient updates, just from prompts.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C2+-+Large+Language+Models.pdf",
        "C2+-+Large+Language+Models.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C2+-+Large+Language+Models.pdf",
        "C2+-+Large+Language+Models.pdf"
      ],
      "top1_doc": "C2+-+Large+Language+Models.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 4
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 3
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 10
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 2
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 33
        }
      ]
    },
    "metrics": {
      "f1": 0.0,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.2049509733915329,
      "latency_seconds": 5.574024499999723,
      "input_tokens": 1607,
      "output_tokens": 213,
      "total_tokens": 1820,
      "input_cost": 0.00200875,
      "output_cost": 0.00213,
      "total_cost": 0.00413875
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C2+-+Large+Language+Models.pdf",
    "question": "Why are long context windows not equivalent to real memory?",
    "gold_answer": "Long contexts only retain information within a single session and do not create persistent knowledge across tasks.",
    "answer": "Because a larger context window only carries over tokens within a single session; once the interaction ends the agent is stateless and nothing persists. This is an illusion of memory—it doesn’t create a persistent, evolving state or long‑term understanding, and longer contexts often degrade quality and increase compute, leading to slower, more error‑prone behavior.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C2+-+Large+Language+Models.pdf",
        "C6+-+Agent+Memory.pdf",
        "C2+-+Large+Language+Models.pdf",
        "C2+-+Large+Language+Models.pdf",
        "C6+-+Agent+Memory.pdf"
      ],
      "top1_doc": "C2+-+Large+Language+Models.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 27
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 2
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 28
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 20
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 1
        }
      ]
    },
    "metrics": {
      "f1": 0.27692307692307694,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.6715762615203857,
      "latency_seconds": 8.591935899999953,
      "input_tokens": 1546,
      "output_tokens": 398,
      "total_tokens": 1944,
      "input_cost": 0.0019325000000000002,
      "output_cost": 0.00398,
      "total_cost": 0.0059125
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C2+-+Large+Language+Models.pdf",
    "question": "What is the purpose of tokenization in an LLM pipeline?",
    "gold_answer": "Tokenization converts input text into tokens that the model can embed and process.",
    "answer": "Tokenization converts raw text into a sequence of discrete tokens (IDs) the model can embed and process. It provides the “next token” units the LLM is trained to predict during pre-training and uses at inference.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C2+-+Large+Language+Models.pdf",
        "C2+-+Large+Language+Models.pdf",
        "C2+-+Large+Language+Models.pdf",
        "C2+-+Large+Language+Models.pdf",
        "C2+-+Large+Language+Models.pdf"
      ],
      "top1_doc": "C2+-+Large+Language+Models.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 31
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 2
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 30
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 28
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 3
        }
      ]
    },
    "metrics": {
      "f1": 0.4888888888888889,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.8005052804946899,
      "latency_seconds": 15.34023519999937,
      "input_tokens": 1559,
      "output_tokens": 629,
      "total_tokens": 2188,
      "input_cost": 0.00194875,
      "output_cost": 0.0062900000000000005,
      "total_cost": 0.00823875
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C3+-+Introduction+to+Agents.pdf",
    "question": "What distinguishes an agent from a chatbot?",
    "gold_answer": "Agents take actions using tools and pursue goals, while chatbots mainly produce text responses.",
    "answer": "An agent is an autonomous, goal-driven system that perceives, plans, and takes actions (often using tools). A chatbot primarily conducts conversation and doesn’t act beyond dialogue.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C3+-+Introduction+to+Agents.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C8+-+Multi-Agent+Systems.pdf"
      ],
      "top1_doc": "C3+-+Introduction+to+Agents.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 2
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 15
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 1
        },
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 7
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 6
        }
      ]
    },
    "metrics": {
      "f1": 0.2105263157894737,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.706959068775177,
      "latency_seconds": 5.054245600000286,
      "input_tokens": 1595,
      "output_tokens": 236,
      "total_tokens": 1831,
      "input_cost": 0.00199375,
      "output_cost": 0.00236,
      "total_cost": 0.00435375
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C3+-+Introduction+to+Agents.pdf",
    "question": "What are the five key components of an AI agent listed in the chapter?",
    "gold_answer": "Multiple LLM calls, tool use, environment interaction, a planner, and autonomy.",
    "answer": "Multiple Calls to LLMs; LLMs with the Ability to Use Tools; An Environment Where LLMs Interact; A Planner Coordinating Activities; and Autonomy.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C3+-+Introduction+to+Agents.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C1+-+Introduction+to+GenAI.pdf",
        "C3+-+Introduction+to+Agents.pdf"
      ],
      "top1_doc": "C3+-+Introduction+to+Agents.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 2
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 0
        },
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 15
        },
        {
          "document": "C1+-+Introduction+to+GenAI.pdf",
          "chunk_id": 0
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 3
        }
      ]
    },
    "metrics": {
      "f1": 0.5333333333333333,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.8481029868125916,
      "latency_seconds": 4.3142067000007955,
      "input_tokens": 1681,
      "output_tokens": 301,
      "total_tokens": 1982,
      "input_cost": 0.0021012500000000003,
      "output_cost": 0.00301,
      "total_cost": 0.00511125
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C3+-+Introduction+to+Agents.pdf",
    "question": "How does reinforcement learning relate to agents?",
    "gold_answer": "RL models agents by having them interact with an environment and learn through rewards.",
    "answer": "Reinforcement learning is the framework by which an agent learns through trial and error in an environment, using rewards to update a policy that maps states to actions to maximize cumulative reward. It formalizes the agent–environment loop with state, action, reward, and policy.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C3+-+Introduction+to+Agents.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C3+-+Introduction+to+Agents.pdf"
      ],
      "top1_doc": "C3+-+Introduction+to+Agents.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 4
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 0
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 6
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 5
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 1
        }
      ]
    },
    "metrics": {
      "f1": 0.28,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.6409098505973816,
      "latency_seconds": 7.617887000000337,
      "input_tokens": 1633,
      "output_tokens": 383,
      "total_tokens": 2016,
      "input_cost": 0.00204125,
      "output_cost": 0.00383,
      "total_cost": 0.00587125
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C3+-+Introduction+to+Agents.pdf",
    "question": "Why did modern LLMs enable advanced agentic systems?",
    "gold_answer": "Because LLMs provide reasoning, planning, generalization, and natural language understanding capabilities.",
    "answer": "Because modern LLMs provide strong language understanding/generation, chain-of-thought reasoning, cross-domain generalization, and multi-step planning with feedback, they unlocked true agentic behavior. Embedding them in feedback loops turns models from static responders into adaptive, goal-driven agents that can act (and use tools) on open-ended tasks.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C3+-+Introduction+to+Agents.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C2+-+Large+Language+Models.pdf",
        "C2+-+Large+Language+Models.pdf"
      ],
      "top1_doc": "C3+-+Introduction+to+Agents.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 1
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 10
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 9
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 3
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 7
        }
      ]
    },
    "metrics": {
      "f1": 0.3018867924528302,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.6997805237770081,
      "latency_seconds": 9.038881100001163,
      "input_tokens": 1661,
      "output_tokens": 460,
      "total_tokens": 2121,
      "input_cost": 0.00207625,
      "output_cost": 0.0046,
      "total_cost": 0.00667625
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C3+-+Introduction+to+Agents.pdf",
    "question": "What does it mean for an agent to be goal-oriented?",
    "gold_answer": "It means the agent selects actions not just to respond but to achieve a defined outcome.",
    "answer": "Goal-oriented means the agent’s behavior is driven by explicit objectives: it perceives, plans, and acts to make progress toward those goals, not just follow a fixed script. In RL terms, this often means choosing actions to maximize cumulative reward.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C3+-+Introduction+to+Agents.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C3+-+Introduction+to+Agents.pdf"
      ],
      "top1_doc": "C3+-+Introduction+to+Agents.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 2
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 11
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 6
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 3
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 1
        }
      ]
    },
    "metrics": {
      "f1": 0.3076923076923077,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.6487341523170471,
      "latency_seconds": 24.486005300001125,
      "input_tokens": 1697,
      "output_tokens": 506,
      "total_tokens": 2203,
      "input_cost": 0.0021212500000000003,
      "output_cost": 0.00506,
      "total_cost": 0.00718125
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
    "question": "What are the two major components that define an agent's mind?",
    "gold_answer": "Agent persona and thinking mechanism.",
    "answer": "Agent Persona and the Thinking Mechanism.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C8+-+Multi-Agent+Systems.pdf",
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C3+-+Introduction+to+Agents.pdf"
      ],
      "top1_doc": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 5
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 7
        },
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 0
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 2
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 1
        }
      ]
    },
    "metrics": {
      "f1": 0.9090909090909091,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": true,
      "semantic_similarity": 0.895605206489563,
      "latency_seconds": 4.609467299998869,
      "input_tokens": 1700,
      "output_tokens": 145,
      "total_tokens": 1845,
      "input_cost": 0.002125,
      "output_cost": 0.00145,
      "total_cost": 0.003575
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
    "question": "What does temperature control in model configuration?",
    "gold_answer": "Temperature controls randomness in token selection and influences creativity versus determinism.",
    "answer": "Temperature controls the randomness/creativity of the model’s token selection. Low values (≈0.0–0.3) produce deterministic, focused, predictable responses, while high values (≈0.7–1.0) yield more diverse, surprising outputs.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C2+-+Large+Language+Models.pdf",
        "C2+-+Large+Language+Models.pdf"
      ],
      "top1_doc": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 1
        },
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 4
        },
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 2
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 19
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 25
        }
      ]
    },
    "metrics": {
      "f1": 0.2285714285714286,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.814148485660553,
      "latency_seconds": 6.255429899998489,
      "input_tokens": 1570,
      "output_tokens": 255,
      "total_tokens": 1825,
      "input_cost": 0.0019625000000000003,
      "output_cost": 0.00255,
      "total_cost": 0.004512500000000001
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
    "question": "What is a Chain-of-Thought reasoning mechanism?",
    "gold_answer": "A reasoning method where the model generates step-by-step explanations before answering.",
    "answer": "Chain-of-Thought (CoT) is a reasoning approach where the model breaks a complex problem into smaller, logical intermediate steps and explains them step-by-step before giving the final answer, mimicking human gradual reasoning. It can be induced via few-shot or zero-shot examples, and automated through Auto-CoT.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C2+-+Large+Language+Models.pdf"
      ],
      "top1_doc": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 12
        },
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 13
        },
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 11
        },
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 10
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 19
        }
      ]
    },
    "metrics": {
      "f1": 0.27450980392156865,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.5433422923088074,
      "latency_seconds": 8.781703000000562,
      "input_tokens": 1615,
      "output_tokens": 455,
      "total_tokens": 2070,
      "input_cost": 0.00201875,
      "output_cost": 0.00455,
      "total_cost": 0.00656875
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
    "question": "What does a high frequency penalty encourage?",
    "gold_answer": "It encourages reduced repetition by penalizing tokens already used frequently.",
    "answer": "A high frequency penalty discourages reusing words, promoting more diverse, less repetitive output—useful for essays, articles, and research.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C7+-+Finetuning.pdf",
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C7+-+Finetuning.pdf",
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf"
      ],
      "top1_doc": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 3
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 2
        },
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 2
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 3
        },
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 4
        }
      ]
    },
    "metrics": {
      "f1": 0.0,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.620832085609436,
      "latency_seconds": 8.637491800000134,
      "input_tokens": 1595,
      "output_tokens": 229,
      "total_tokens": 1824,
      "input_cost": 0.00199375,
      "output_cost": 0.00229,
      "total_cost": 0.004283749999999999
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
    "question": "Why do different prompts create different agent personas?",
    "gold_answer": "Because the LLM conditions its behavior on the instructions, shaping style, tone, and reasoning patterns.",
    "answer": "Because prompts act as behavioral blueprints (policies) that specify the agent’s role, tone, vocabulary, response style, and values, the LLM conditions on them and behaves accordingly. Different prompts—and configurations like temperature and penalties—therefore yield different expressed personas.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf"
      ],
      "top1_doc": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 6
        },
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 7
        },
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 5
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 13
        },
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 15
        }
      ]
    },
    "metrics": {
      "f1": 0.3404255319148936,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.5811744928359985,
      "latency_seconds": 7.3269870999993145,
      "input_tokens": 1497,
      "output_tokens": 383,
      "total_tokens": 1880,
      "input_cost": 0.00187125,
      "output_cost": 0.00383,
      "total_cost": 0.00570125
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C5+-+Agent+Tools.pdf",
    "question": "What are agent tools?",
    "gold_answer": "Functions provided to an LLM that enable actions or computations beyond the model’s internal capabilities.",
    "answer": "Agent tools are functions the LLM can call to perform actions on its behalf—operations that have effects or require computation beyond the model’s own capabilities. They include a description, a callable, typed arguments, and optionally typed outputs so the model knows when and how to use them.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C5+-+Agent+Tools.pdf",
        "C5+-+Agent+Tools.pdf",
        "C8+-+Multi-Agent+Systems.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C5+-+Agent+Tools.pdf"
      ],
      "top1_doc": "C5+-+Agent+Tools.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 0
        },
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 11
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 6
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 15
        },
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 1
        }
      ]
    },
    "metrics": {
      "f1": 0.35714285714285715,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.6866199374198914,
      "latency_seconds": 6.2380686000014975,
      "input_tokens": 1591,
      "output_tokens": 257,
      "total_tokens": 1848,
      "input_cost": 0.00198875,
      "output_cost": 0.0025700000000000002,
      "total_cost": 0.00455875
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C5+-+Agent+Tools.pdf",
    "question": "Why do agents need tools for real-time information?",
    "gold_answer": "LLMs lack up-to-date knowledge and can hallucinate without retrieving external information.",
    "answer": "Because an LLM’s internal knowledge only covers events prior to its training, it can’t access current data. Tools provide real-time information, preventing the model from hallucinating about up-to-date facts (e.g., today’s weather).",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C5+-+Agent+Tools.pdf",
        "C5+-+Agent+Tools.pdf",
        "C8+-+Multi-Agent+Systems.pdf",
        "C5+-+Agent+Tools.pdf",
        "C3+-+Introduction+to+Agents.pdf"
      ],
      "top1_doc": "C5+-+Agent+Tools.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 0
        },
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 1
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 1
        },
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 11
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 15
        }
      ]
    },
    "metrics": {
      "f1": 0.13953488372093023,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.6824795007705688,
      "latency_seconds": 6.516553899999053,
      "input_tokens": 1612,
      "output_tokens": 312,
      "total_tokens": 1924,
      "input_cost": 0.0020150000000000003,
      "output_cost": 0.00312,
      "total_cost": 0.005135000000000001
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C5+-+Agent+Tools.pdf",
    "question": "What are the three main characteristics of a good tool?",
    "gold_answer": "A well-defined API, isolated functionality, and deterministic behavior.",
    "answer": "- Single-purpose with a clear, well-defined objective.\n- Clearly described so the agent knows exactly when to use it.\n- Exposed as a callable with a well-typed interface (arguments and optional outputs).",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C5+-+Agent+Tools.pdf",
        "C5+-+Agent+Tools.pdf",
        "C5+-+Agent+Tools.pdf",
        "C5+-+Agent+Tools.pdf",
        "C5+-+Agent+Tools.pdf"
      ],
      "top1_doc": "C5+-+Agent+Tools.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 9
        },
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 10
        },
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 1
        },
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 0
        },
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 11
        }
      ]
    },
    "metrics": {
      "f1": 0.1764705882352941,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.5847089290618896,
      "latency_seconds": 26.816717299998345,
      "input_tokens": 1602,
      "output_tokens": 1395,
      "total_tokens": 2997,
      "input_cost": 0.0020025,
      "output_cost": 0.01395,
      "total_cost": 0.0159525
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C5+-+Agent+Tools.pdf",
    "question": "What protocol standardizes how tools communicate with LLMs?",
    "gold_answer": "The Model Context Protocol (MCP).",
    "answer": "The Model Context Protocol (MCP) standardizes how tools communicate with LLMs, defining tool schemas, parameters, and output formats for consistent, validated action calls.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C3+-+Introduction+to+Agents.pdf",
        "C5+-+Agent+Tools.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C5+-+Agent+Tools.pdf",
        "C5+-+Agent+Tools.pdf"
      ],
      "top1_doc": "C3+-+Introduction+to+Agents.pdf",
      "top1_matches_gold": false,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 12
        },
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 1
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 10
        },
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 12
        },
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 8
        }
      ]
    },
    "metrics": {
      "f1": 0.3571428571428571,
      "exact_match_normalized": false,
      "contains_match": true,
      "is_correct": true,
      "semantic_similarity": 0.8387960195541382,
      "latency_seconds": 3.4451567999985855,
      "input_tokens": 1584,
      "output_tokens": 235,
      "total_tokens": 1819,
      "input_cost": 0.00198,
      "output_cost": 0.00235,
      "total_cost": 0.0043300000000000005
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C5+-+Agent+Tools.pdf",
    "question": "Why do tools reduce token usage?",
    "gold_answer": "Because they return structured, concise data instead of requiring the LLM to process raw text.",
    "answer": "I don't know from the course materials. The slides discuss efficiency, reliability, and prompt/schema clarity for tools, but they don’t explicitly connect tools to reduced token usage.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C5+-+Agent+Tools.pdf",
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C5+-+Agent+Tools.pdf",
        "C5+-+Agent+Tools.pdf",
        "C5+-+Agent+Tools.pdf"
      ],
      "top1_doc": "C5+-+Agent+Tools.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 9
        },
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 2
        },
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 10
        },
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 11
        },
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 1
        }
      ]
    },
    "metrics": {
      "f1": 0.15,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.2964574098587036,
      "latency_seconds": 18.278697800000373,
      "input_tokens": 1588,
      "output_tokens": 939,
      "total_tokens": 2527,
      "input_cost": 0.0019850000000000002,
      "output_cost": 0.009389999999999999,
      "total_cost": 0.011375
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C6+-+Agent+Memory.pdf",
    "question": "Why do agents need memory?",
    "gold_answer": "To maintain context across interactions, enabling continuity and personalized behavior.",
    "answer": "Agents need memory because LLM-based agents are stateless and forget past interactions beyond the prompt, so they can’t carry context across tasks or sessions. Memory provides a persistent internal state that retains and recalls relevant information over time, enabling continuity, personalization, learning from the past, and more advanced autonomous behavior.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf"
      ],
      "top1_doc": "C6+-+Agent+Memory.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 1
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 30
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 4
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 0
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 28
        }
      ]
    },
    "metrics": {
      "f1": 0.25925925925925924,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.4759726822376251,
      "latency_seconds": 5.706224600000496,
      "input_tokens": 1334,
      "output_tokens": 262,
      "total_tokens": 1596,
      "input_cost": 0.0016675000000000001,
      "output_cost": 0.0026200000000000004,
      "total_cost": 0.0042875000000000005
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C6+-+Agent+Memory.pdf",
    "question": "What is short-term memory in an agent?",
    "gold_answer": "It stores the current conversation or task context within a single session.",
    "answer": "Short-term memory is the agent’s working context for the current task or conversation—recent interactions and observations kept in-memory (e.g., a buffer or summarized history). It maintains coherence within a session but isn’t persisted across sessions.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf"
      ],
      "top1_doc": "C6+-+Agent+Memory.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 4
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 28
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 1
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 5
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 27
        }
      ]
    },
    "metrics": {
      "f1": 0.4090909090909091,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.5933569669723511,
      "latency_seconds": 9.951768000000811,
      "input_tokens": 1542,
      "output_tokens": 503,
      "total_tokens": 2045,
      "input_cost": 0.0019275000000000002,
      "output_cost": 0.00503,
      "total_cost": 0.0069575
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C6+-+Agent+Memory.pdf",
    "question": "What is semantic long-term memory?",
    "gold_answer": "Memory that stores general facts, concepts, and stable user preferences.",
    "answer": "Semantic long-term memory stores facts and general knowledge (as opposed to specific experiences). In the course, it’s described as the place where facts are written and later retrieved into working memory for decision-making.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf"
      ],
      "top1_doc": "C6+-+Agent+Memory.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 3
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 4
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 5
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 2
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 6
        }
      ]
    },
    "metrics": {
      "f1": 0.2631578947368421,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.6674031019210815,
      "latency_seconds": 8.817017100000157,
      "input_tokens": 1604,
      "output_tokens": 433,
      "total_tokens": 2037,
      "input_cost": 0.0020050000000000003,
      "output_cost": 0.00433,
      "total_cost": 0.006335
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C6+-+Agent+Memory.pdf",
    "question": "Why don’t large context windows replace memory?",
    "gold_answer": "Because context windows are per-session and do not enable persistence across tasks or time.",
    "answer": "Because large context windows only retain information within a single prompt/session, LLMs remain stateless and lack the persistent, evolving internal state that true memory provides. Also, very long contexts are expensive (quadratic attention) and often degrade quality, causing slower, more error-prone responses.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C2+-+Large+Language+Models.pdf",
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf",
        "C2+-+Large+Language+Models.pdf",
        "C2+-+Large+Language+Models.pdf"
      ],
      "top1_doc": "C2+-+Large+Language+Models.pdf",
      "top1_matches_gold": false,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 27
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 2
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 1
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 28
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 20
        }
      ]
    },
    "metrics": {
      "f1": 0.18181818181818182,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.6537216901779175,
      "latency_seconds": 9.318045500000153,
      "input_tokens": 1544,
      "output_tokens": 515,
      "total_tokens": 2059,
      "input_cost": 0.00193,
      "output_cost": 0.00515,
      "total_cost": 0.00708
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C6+-+Agent+Memory.pdf",
    "question": "What is the role of a retriever in RAG?",
    "gold_answer": "The retriever finds semantically relevant information from external sources based on embeddings.",
    "answer": "In RAG, the retriever searches the knowledge base (often via indexed embeddings) to fetch the most relevant chunks/documents for a query. It returns these results to be injected into the LLM’s prompt to ground the generation.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf"
      ],
      "top1_doc": "C6+-+Agent+Memory.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 9
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 8
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 7
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 17
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 10
        }
      ]
    },
    "metrics": {
      "f1": 0.19047619047619044,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.6194494366645813,
      "latency_seconds": 7.00009990000035,
      "input_tokens": 1661,
      "output_tokens": 312,
      "total_tokens": 1973,
      "input_cost": 0.00207625,
      "output_cost": 0.00312,
      "total_cost": 0.0051962499999999995
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C7+-+Finetuning.pdf",
    "question": "What is the main purpose of fine-tuning?",
    "gold_answer": "To adapt a general LLM to specific tasks, domains, or behaviors using specialized training data.",
    "answer": "To adapt a general model to your specific domain, tasks, and behaviors by training it a bit more on your own, task-specific data. This bridges the gap between general knowledge and your needs, making the model better at your particular problem and aligning it with desired behaviors.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C7+-+Finetuning.pdf",
        "C7+-+Finetuning.pdf",
        "C7+-+Finetuning.pdf",
        "C7+-+Finetuning.pdf",
        "C7+-+Finetuning.pdf"
      ],
      "top1_doc": "C7+-+Finetuning.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 0
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 2
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 1
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 3
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 10
        }
      ]
    },
    "metrics": {
      "f1": 0.36734693877551017,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.7392627000808716,
      "latency_seconds": 7.184538299999986,
      "input_tokens": 1448,
      "output_tokens": 319,
      "total_tokens": 1767,
      "input_cost": 0.00181,
      "output_cost": 0.00319,
      "total_cost": 0.005
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C7+-+Finetuning.pdf",
    "question": "Why is full fine-tuning expensive?",
    "gold_answer": "Because it updates all model parameters, requiring significant compute and memory.",
    "answer": "Because it updates all model parameters—often billions—full fine-tuning requires FP16 training with very high GPU memory and compute. You must run full forward/backward passes and optimize over many batches and epochs, driving up time and cost.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C7+-+Finetuning.pdf",
        "C7+-+Finetuning.pdf",
        "C7+-+Finetuning.pdf",
        "C7+-+Finetuning.pdf",
        "C7+-+Finetuning.pdf"
      ],
      "top1_doc": "C7+-+Finetuning.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 2
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 0
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 3
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 1
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 8
        }
      ]
    },
    "metrics": {
      "f1": 0.36363636363636365,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.7058793306350708,
      "latency_seconds": 7.818109499999991,
      "input_tokens": 1669,
      "output_tokens": 377,
      "total_tokens": 2046,
      "input_cost": 0.00208625,
      "output_cost": 0.00377,
      "total_cost": 0.00585625
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C7+-+Finetuning.pdf",
    "question": "What does LoRA modify during fine-tuning?",
    "gold_answer": "LoRA inserts low-rank adapters to modify only a small subset of parameters efficiently.",
    "answer": "LoRA keeps the base model weights frozen and adds low-rank adapter matrices to selected target modules. During fine-tuning, only these small adapter parameters are updated to adjust those layers.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C7+-+Finetuning.pdf",
        "C7+-+Finetuning.pdf",
        "C7+-+Finetuning.pdf",
        "C7+-+Finetuning.pdf",
        "C2+-+Large+Language+Models.pdf"
      ],
      "top1_doc": "C7+-+Finetuning.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 7
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 6
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 8
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 0
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 33
        }
      ]
    },
    "metrics": {
      "f1": 0.3,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.6776849627494812,
      "latency_seconds": 6.62919299999885,
      "input_tokens": 1701,
      "output_tokens": 302,
      "total_tokens": 2003,
      "input_cost": 0.00212625,
      "output_cost": 0.00302,
      "total_cost": 0.00514625
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C7+-+Finetuning.pdf",
    "question": "What is one reason to prefer RAG over fine-tuning?",
    "gold_answer": "RAG integrates real-time external knowledge without updating model weights.",
    "answer": "RAG is preferred when you need up-to-date or very large knowledge bases, since it retrieves current external documents instead of retraining the model.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C7+-+Finetuning.pdf",
        "C7+-+Finetuning.pdf",
        "C7+-+Finetuning.pdf",
        "C7+-+Finetuning.pdf",
        "C7+-+Finetuning.pdf"
      ],
      "top1_doc": "C7+-+Finetuning.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 2
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 1
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 10
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 0
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 3
        }
      ]
    },
    "metrics": {
      "f1": 0.25,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.7286902070045471,
      "latency_seconds": 8.108060200000182,
      "input_tokens": 1451,
      "output_tokens": 358,
      "total_tokens": 1809,
      "input_cost": 0.00181375,
      "output_cost": 0.00358,
      "total_cost": 0.00539375
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C7+-+Finetuning.pdf",
    "question": "What is the objective of evaluation after fine-tuning?",
    "gold_answer": "To ensure the fine-tuned model performs well on technical metrics and practical use cases.",
    "answer": "To verify that the fine-tuned model achieves the desired improvements on the target task and meets business goals, using both technical and business metrics.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C7+-+Finetuning.pdf",
        "C7+-+Finetuning.pdf",
        "C7+-+Finetuning.pdf",
        "C7+-+Finetuning.pdf",
        "C2+-+Large+Language+Models.pdf"
      ],
      "top1_doc": "C7+-+Finetuning.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 0
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 1
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 3
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 2
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 33
        }
      ]
    },
    "metrics": {
      "f1": 0.47058823529411764,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.7481414079666138,
      "latency_seconds": 11.435150200000862,
      "input_tokens": 1639,
      "output_tokens": 549,
      "total_tokens": 2188,
      "input_cost": 0.0020487500000000002,
      "output_cost": 0.005490000000000001,
      "total_cost": 0.007538750000000001
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C8+-+Multi-Agent+Systems.pdf",
    "question": "Why are multi-agent systems needed for complex tasks?",
    "gold_answer": "Because complex tasks require decomposition into specialized subtasks handled by different agents.",
    "answer": "Because as task complexity grows, a single “helpful assistant” can’t reliably handle everything—LLMs struggle with complex inputs and quality plummets when one agent holds all tools. Multi‑agent designs split work into manageable subtasks with specialized roles (planner, designer, programmer, QA, DevOps), improving focus and performance and fitting heterogeneous tools, pipelines, and safety separation.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C8+-+Multi-Agent+Systems.pdf",
        "C8+-+Multi-Agent+Systems.pdf",
        "C8+-+Multi-Agent+Systems.pdf",
        "C8+-+Multi-Agent+Systems.pdf",
        "C8+-+Multi-Agent+Systems.pdf"
      ],
      "top1_doc": "C8+-+Multi-Agent+Systems.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 1
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 3
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 2
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 0
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 9
        }
      ]
    },
    "metrics": {
      "f1": 0.16666666666666669,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.6861326694488525,
      "latency_seconds": 9.426238599999124,
      "input_tokens": 1533,
      "output_tokens": 406,
      "total_tokens": 1939,
      "input_cost": 0.00191625,
      "output_cost": 0.00406,
      "total_cost": 0.005976250000000001
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C8+-+Multi-Agent+Systems.pdf",
    "question": "What is an example of a natural pipeline that benefits from MAS?",
    "gold_answer": "ETL → analyze → generate → verify → publish.",
    "answer": "An ETL → analyze → generate → verify → publish workflow.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C8+-+Multi-Agent+Systems.pdf",
        "C8+-+Multi-Agent+Systems.pdf",
        "C6+-+Agent+Memory.pdf",
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C8+-+Multi-Agent+Systems.pdf"
      ],
      "top1_doc": "C8+-+Multi-Agent+Systems.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 2
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 0
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 18
        },
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 17
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 1
        }
      ]
    },
    "metrics": {
      "f1": 0.8571428571428571,
      "exact_match_normalized": false,
      "contains_match": true,
      "is_correct": true,
      "semantic_similarity": 0.9143211245536804,
      "latency_seconds": 5.863359999999375,
      "input_tokens": 1585,
      "output_tokens": 214,
      "total_tokens": 1799,
      "input_cost": 0.00198125,
      "output_cost": 0.00214,
      "total_cost": 0.00412125
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C8+-+Multi-Agent+Systems.pdf",
    "question": "What is a reason to avoid multi-agent systems?",
    "gold_answer": "When orchestration complexity outweighs the benefits.",
    "answer": "Avoid multi-agent systems when the orchestration complexity outweighs the benefits or when latency/cost budgets are tight.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C8+-+Multi-Agent+Systems.pdf",
        "C8+-+Multi-Agent+Systems.pdf",
        "C8+-+Multi-Agent+Systems.pdf",
        "C8+-+Multi-Agent+Systems.pdf",
        "C8+-+Multi-Agent+Systems.pdf"
      ],
      "top1_doc": "C8+-+Multi-Agent+Systems.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 3
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 2
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 1
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 9
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 0
        }
      ]
    },
    "metrics": {
      "f1": 0.6,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.678556501865387,
      "latency_seconds": 5.694605699998647,
      "input_tokens": 1533,
      "output_tokens": 223,
      "total_tokens": 1756,
      "input_cost": 0.00191625,
      "output_cost": 0.00223,
      "total_cost": 0.004146250000000001
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C8+-+Multi-Agent+Systems.pdf",
    "question": "What is agent-to-agent communication used for?",
    "gold_answer": "To coordinate tasks and exchange intermediate results between agents.",
    "answer": "Agent-to-agent communication enables interoperability and coordination in multi-agent systems. It lets a client agent discover specialists and delegate tasks, exchange messages (text/JSON/files), and receive artifacts as results over standard web protocols.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C8+-+Multi-Agent+Systems.pdf",
        "C8+-+Multi-Agent+Systems.pdf",
        "C5+-+Agent+Tools.pdf",
        "C8+-+Multi-Agent+Systems.pdf",
        "C8+-+Multi-Agent+Systems.pdf"
      ],
      "top1_doc": "C8+-+Multi-Agent+Systems.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 6
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 7
        },
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 12
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 3
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 1
        }
      ]
    },
    "metrics": {
      "f1": 0.21052631578947367,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.6560701727867126,
      "latency_seconds": 6.235533699998996,
      "input_tokens": 1590,
      "output_tokens": 371,
      "total_tokens": 1961,
      "input_cost": 0.0019875,
      "output_cost": 0.00371,
      "total_cost": 0.0056975
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C8+-+Multi-Agent+Systems.pdf",
    "question": "Why might redundancy be used in MAS?",
    "gold_answer": "To improve reliability by having multiple agents independently produce or verify results.",
    "answer": "From the course materials: redundancy is used to enable ensembles—having multiple agents tackle the same task—which is one of the key reasons to adopt MAS (alongside deliberation and safety separation). I don't know from the course materials beyond that.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C8+-+Multi-Agent+Systems.pdf",
        "C8+-+Multi-Agent+Systems.pdf",
        "C8+-+Multi-Agent+Systems.pdf",
        "C6+-+Agent+Memory.pdf",
        "C5+-+Agent+Tools.pdf"
      ],
      "top1_doc": "C8+-+Multi-Agent+Systems.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 0
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 2
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 1
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 8
        },
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 9
        }
      ]
    },
    "metrics": {
      "f1": 0.13953488372093023,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.404380738735199,
      "latency_seconds": 16.4507365999998,
      "input_tokens": 1577,
      "output_tokens": 697,
      "total_tokens": 2274,
      "input_cost": 0.00197125,
      "output_cost": 0.00697,
      "total_cost": 0.00894125
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C8+-+Multi-Agent+Systems.pdf",
    "question": "What problem arises if one agent tries to do everything?",
    "gold_answer": "Performance drops due to cognitive overload and lack of specialization.",
    "answer": "The quality of output plummets. A single “do‑everything” agent struggles with complex inputs and roles, so it can’t execute complex tasks as accurately as specialized sub‑agents.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C8+-+Multi-Agent+Systems.pdf",
        "C8+-+Multi-Agent+Systems.pdf",
        "C8+-+Multi-Agent+Systems.pdf",
        "C8+-+Multi-Agent+Systems.pdf",
        "C8+-+Multi-Agent+Systems.pdf"
      ],
      "top1_doc": "C8+-+Multi-Agent+Systems.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 1
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 9
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 3
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 0
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 4
        }
      ]
    },
    "metrics": {
      "f1": 0.11764705882352941,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.5216374397277832,
      "latency_seconds": 5.209762899999987,
      "input_tokens": 1486,
      "output_tokens": 239,
      "total_tokens": 1725,
      "input_cost": 0.0018575,
      "output_cost": 0.0023899999999999998,
      "total_cost": 0.0042474999999999995
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C5+-+Agent+Tools.pdf",
    "question": "How do tools enhance safety in agent systems?",
    "gold_answer": "By constraining outputs and validating inputs, ensuring predictable and secure operations.",
    "answer": "Tools enhance safety by constraining agent actions to well-defined, typed functions and reducing hallucinations (e.g., using tools for up-to-date data). They also add safeguards—schema validation, fallbacks, retries, and tool metadata—so failures like 404s or malformed responses don’t cause the agent to get stuck.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C5+-+Agent+Tools.pdf",
        "C5+-+Agent+Tools.pdf",
        "C3+-+Introduction+to+Agents.pdf",
        "C8+-+Multi-Agent+Systems.pdf",
        "C5+-+Agent+Tools.pdf"
      ],
      "top1_doc": "C5+-+Agent+Tools.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 1
        },
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 0
        },
        {
          "document": "C3+-+Introduction+to+Agents.pdf",
          "chunk_id": 15
        },
        {
          "document": "C8+-+Multi-Agent+Systems.pdf",
          "chunk_id": 1
        },
        {
          "document": "C5+-+Agent+Tools.pdf",
          "chunk_id": 11
        }
      ]
    },
    "metrics": {
      "f1": 0.12244897959183675,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.42953598499298096,
      "latency_seconds": 15.101455399999395,
      "input_tokens": 1611,
      "output_tokens": 778,
      "total_tokens": 2389,
      "input_cost": 0.00201375,
      "output_cost": 0.0077800000000000005,
      "total_cost": 0.00979375
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C6+-+Agent+Memory.pdf",
    "question": "What is episodic memory?",
    "gold_answer": "Memory of specific events or interactions that happened previously, including when and where they occurred.",
    "answer": "I don't know from the course materials.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf",
        "C6+-+Agent+Memory.pdf"
      ],
      "top1_doc": "C6+-+Agent+Memory.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 3
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 1
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 0
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 4
        },
        {
          "document": "C6+-+Agent+Memory.pdf",
          "chunk_id": 6
        }
      ]
    },
    "metrics": {
      "f1": 0.0,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.14894482493400574,
      "latency_seconds": 9.011076400000093,
      "input_tokens": 1579,
      "output_tokens": 401,
      "total_tokens": 1980,
      "input_cost": 0.00197375,
      "output_cost": 0.0040100000000000005,
      "total_cost": 0.00598375
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
    "question": "How does top-p sampling work?",
    "gold_answer": "It restricts token selection to a subset of tokens whose cumulative probability mass is below a threshold p.",
    "answer": "Top-p (nucleus) sampling picks from the smallest set of tokens whose cumulative probability exceeds a threshold p, then samples within that set. Low p (e.g., 0.1) restricts to only the most likely tokens; high p (e.g., 0.9) allows a broader, more diverse set.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
        "C7+-+Finetuning.pdf",
        "C4+-+Agent+Personas++Thinking+Mechanisms.pdf"
      ],
      "top1_doc": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 2
        },
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 3
        },
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 4
        },
        {
          "document": "C7+-+Finetuning.pdf",
          "chunk_id": 2
        },
        {
          "document": "C4+-+Agent+Personas++Thinking+Mechanisms.pdf",
          "chunk_id": 11
        }
      ]
    },
    "metrics": {
      "f1": 0.3846153846153846,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.6959227323532104,
      "latency_seconds": 6.239382400000977,
      "input_tokens": 1575,
      "output_tokens": 332,
      "total_tokens": 1907,
      "input_cost": 0.00196875,
      "output_cost": 0.00332,
      "total_cost": 0.00528875
    }
  },
  {
    "provider": "openai",
    "model": "gpt-5",
    "repeat_id": 0,
    "document": "C2+-+Large+Language+Models.pdf",
    "question": "What is one task LLMs can perform using only prompting without parameter updates?",
    "gold_answer": "In-context learning.",
    "answer": "They can, for example, write syntactically correct and functional code purely from a prompt without any parameter updates.",
    "retrieval": {
      "top_k": 5,
      "retrieved_docs": [
        "C2+-+Large+Language+Models.pdf",
        "C2+-+Large+Language+Models.pdf",
        "C2+-+Large+Language+Models.pdf",
        "C2+-+Large+Language+Models.pdf",
        "C2+-+Large+Language+Models.pdf"
      ],
      "top1_doc": "C2+-+Large+Language+Models.pdf",
      "top1_matches_gold": true,
      "any_match_in_topk": true,
      "chunks_used": [
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 4
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 5
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 6
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 3
        },
        {
          "document": "C2+-+Large+Language+Models.pdf",
          "chunk_id": 33
        }
      ]
    },
    "metrics": {
      "f1": 0.0,
      "exact_match_normalized": false,
      "contains_match": false,
      "is_correct": false,
      "semantic_similarity": 0.2751249074935913,
      "latency_seconds": 8.594625199999427,
      "input_tokens": 1639,
      "output_tokens": 416,
      "total_tokens": 2055,
      "input_cost": 0.0020487500000000002,
      "output_cost": 0.00416,
      "total_cost": 0.00620875
    }
  }
]